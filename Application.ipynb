{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db6555e9",
   "metadata": {},
   "source": [
    "### CSCT Masters Project 23Sep-UFCF9Y-60-M\n",
    "#### Student_Number:23044930\n",
    "#### Title: Interactive Framework for Bias Detection and Mitigation in Machine Learning Models Using Fairness Constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0398daf3",
   "metadata": {},
   "source": [
    "### Content\n",
    " #### 1. Initialisation\n",
    " #### 2. Preprocessing of Data(Load Data).\n",
    " #### 3. Initial Bias Check and Quantification of Bias Percentage.\n",
    " #### 4. Applying Logistic Regression, Decision Tree, Random Forest and Gradient Boosting Models for Bias quantification.\n",
    " #### 5. StreamLit App Interface.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eada532",
   "metadata": {},
   "source": [
    "### 1. Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617dbf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for the project\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from fairlearn.metrics import MetricFrame, selection_rate, true_positive_rate, true_negative_rate, false_positive_rate, false_negative_rate\n",
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity, EqualizedOdds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699e600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility to ensure consistent results across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Dynamic patch to address an issue with the Fairlearn library\n",
    "import fairlearn.reductions._exponentiated_gradient.exponentiated_gradient as eg\n",
    "if hasattr(np, 'PINF'):\n",
    "    np.PINF = np.inf\n",
    "eg.np.PINF = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d8fbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sidebar explanation of fairness constraints\n",
    "st.sidebar.subheader(\"Fairness Constraints Explanations\")\n",
    "st.sidebar.markdown(\"\"\"\n",
    "- **Demographic Parity**: Ensures that the selection rates (e.g., positive predictions) are similar across different groups. It may lead to reduced accuracy because it doesn't account for true outcomes.\n",
    "  \n",
    "- **Equalized Odds**: Ensures that true positive rates and false positive rates are similar across groups. It can improve fairness but might require more complex adjustments to the model.\n",
    "\n",
    "**Recommendations**:\n",
    "- Choose **Logistic Regression** if interpretability is key and the data is not too complex.\n",
    "- Use **Random Forest** or **Gradient Boosting** if you prioritize accuracy and can handle longer training times.\n",
    "- **Demographic Parity** is recommended if ensuring equal opportunity across groups is more important than overall accuracy.\n",
    "- **Equalized Odds** is better if you want to ensure fairness in the model's error rates across groups.\n",
    "- Experiment with different models and fairness constraints to see which combination best meets your needs.\n",
    "\"\"\")\n",
    "\n",
    "# Display an 'About' section to introduce the concept of bias in machine learning models\n",
    "def about_section():\n",
    "    \"\"\"Displays an 'About' section at the top of the app to introduce the concept of bias.\"\"\"\n",
    "    st.markdown(\"# What is Bias?\")\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "        Bias in machine learning refers to the systematic error that leads a model to make incorrect assumptions \n",
    "        about data. This can result from the way data is collected, processed, or how a model is trained. \n",
    "        Our tool provides a quantified figure to measure bias in models, helping you understand how fairly \n",
    "        your model treats different groups defined by sensitive features such as gender, race, or age. And to mitigate the identified bias.\n",
    "    \n",
    "        \n",
    "        This user-friendly interface was developed to check the percentage of bias that may exist in your dataset against sensitive \n",
    "        features like gender, age, race,marital status etc.This interface allows user to upload the data, select the target variable\n",
    "        and sensitive feature and check for the bias percentage and proceed to mitigate the bias based on chosen fairness constriant.\n",
    "        The initial stage of development only allows for target column in binary value analysis\n",
    "        of the data. Future developments would allow you to check and mitigate bias percentage with any target column.\n",
    "        This project was developed as a artefact of the Msc. thesis. Enjoy!\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335b77e0",
   "metadata": {},
   "source": [
    "### 2. Preprocessing of Data (Load Data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393da3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "def load_and_preprocess_data(uploaded_file):\n",
    "    \"\"\"Loads the uploaded CSV file and handles any errors during the process.\n",
    "    \n",
    "    Args:\n",
    "        uploaded_file: The uploaded file object.\n",
    "    \n",
    "    Returns:\n",
    "        data: The loaded data as a pandas DataFrame, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Attempt to load the data from the uploaded file\n",
    "        data = pd.read_csv(uploaded_file)\n",
    "        \n",
    "        # Preprocess the dataset\n",
    "        # Replace placeholder '?' with NaN and drop rows with missing values\n",
    "        data = data.replace('?', pd.NA).dropna()\n",
    "        \n",
    "    except pd.errors.EmptyDataError:\n",
    "        # Display an error if the file is empty or incorrectly formatted\n",
    "        st.error(\"The uploaded file is empty or malformed.\")\n",
    "        return None\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8856f35f",
   "metadata": {},
   "source": [
    "The columns are encoded to numerical values for the purpose of model training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737d652b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical columns and retain the mapping for the sensitive feature\n",
    "def encode_columns(data, sensitive_feature_column):\n",
    "    \"\"\"Encodes categorical columns in the dataset and keeps the mapping for the sensitive feature.\n",
    "    \n",
    "    Args:\n",
    "        data: The dataset as a pandas DataFrame.\n",
    "        sensitive_feature_column: The column name of the sensitive feature.\n",
    "\n",
    "    Returns:\n",
    "        data_encoded: The encoded dataset.\n",
    "        sensitive_feature_mapping: A dictionary mapping encoded values to original categories for the sensitive feature.\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataset to avoid modifying the original data\n",
    "    data_encoded = data.copy()\n",
    "    \n",
    "    # Encode all categorical features except the sensitive feature\n",
    "    for column in data_encoded.columns:\n",
    "        if data_encoded[column].dtype == object and column != sensitive_feature_column:\n",
    "            data_encoded[column] = LabelEncoder().fit_transform(data_encoded[column])\n",
    "    \n",
    "    # Encode the sensitive feature and retain the mapping of the original categories\n",
    "    le = LabelEncoder()\n",
    "    data_encoded[sensitive_feature_column] = le.fit_transform(data_encoded[sensitive_feature_column])\n",
    "    sensitive_feature_mapping = dict(zip(le.transform(le.classes_), le.classes_))\n",
    "    \n",
    "    return data_encoded, sensitive_feature_mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5073d7b6",
   "metadata": {},
   "source": [
    "### Initial Bias Check and Quantification of Bias Percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2459df95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial bias check\n",
    "def check_bias_logistic_regression(data, target_column, sensitive_feature_column, sensitive_feature_mapping):\n",
    "    \"\"\"Performs an initial bias check using Logistic Regression.\n",
    "    \n",
    "    This function trains a Logistic Regression model on the dataset and evaluates bias using \n",
    "    fairness metrics like selection rate, true positive rate, and disparate impact. It also\n",
    "    plots a bar graph to show which group is favored based on the selected metric.\n",
    "    \n",
    "    Args:\n",
    "        data: The dataset as a pandas DataFrame.\n",
    "        target_column: The column name of the target variable.\n",
    "        sensitive_feature_column: The column name of the sensitive feature.\n",
    "        sensitive_feature_mapping: The mapping of encoded values to original categories for the sensitive feature.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary containing the results of the bias analysis, including disparate impact and bias percentage.\n",
    "    \"\"\"\n",
    "    # Split data into training and testing sets, with stratification based on the sensitive feature\n",
    "    X_train, X_test, y_train, y_test, sensitive_train, sensitive_test = train_test_split(\n",
    "        data.drop(target_column, axis=1), data[target_column], data[sensitive_feature_column], \n",
    "        test_size=0.25, random_state=42\n",
    "    )\n",
    "\n",
    "    # Initialize and train the Logistic Regression model on the training set\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the testing set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Define fairness metrics to evaluate the model's performance across different groups\n",
    "    metrics = {\n",
    "        'selection_rate': selection_rate,\n",
    "        'true_positive_rate': true_positive_rate,\n",
    "        'true_negative_rate': true_negative_rate,\n",
    "        'false_positive_rate': false_positive_rate,\n",
    "        'false_negative_rate': false_negative_rate\n",
    "    }\n",
    "\n",
    "    # Create a MetricFrame to evaluate the model's fairness across sensitive groups\n",
    "    metric_frame = MetricFrame(metrics=metrics, y_true=y_test, y_pred=y_pred, sensitive_features=sensitive_test)\n",
    "    \n",
    "    # Get the selection rate by group from the metric frame\n",
    "    sr_by_group = metric_frame.by_group['selection_rate']\n",
    "\n",
    "    # Map the numeric index back to the original category labels\n",
    "    sr_by_group.index = sr_by_group.index.map(lambda x: sensitive_feature_mapping[x])\n",
    "\n",
    "    # Calculate disparate impact and bias percentage for the sensitive feature\n",
    "    disparate_impact = sr_by_group.min() / sr_by_group.max()\n",
    "    bias_percentage = (1 - disparate_impact) * 100\n",
    "\n",
    "    # Plotting the selection rate across different sensitive groups\n",
    "    fig, ax = plt.subplots()\n",
    "    sr_by_group.plot(kind='bar', color='skyblue', ax=ax)\n",
    "    plt.title('Selection Rate by Sensitive Group')\n",
    "    plt.ylabel('Selection Rate')\n",
    "    plt.xlabel('Sensitive Group')\n",
    "    st.pyplot(fig)\n",
    "\n",
    "    # Return the model, metric frame, and calculated bias metrics for further analysis\n",
    "    return {\n",
    "        \"Disparate Impact\": disparate_impact,\n",
    "        \"Bias Percentage\": bias_percentage,\n",
    "        \"Metric Frame\": metric_frame,\n",
    "        \"X_train\": X_train, \"X_test\": X_test, \"y_train\": y_train, \"y_test\": y_test,\n",
    "        \"Sensitive Train\": sensitive_train, \"Sensitive Test\": sensitive_test\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d3e384",
   "metadata": {},
   "source": [
    "### Applying Logistic Regression, Decision Tree, Random Forest and Gradient Boosting for Bias quantification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df51c435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models and apply fairness constraints\n",
    "def apply_fairness_constraints(data, target_column, sensitive_feature_column, constraint_option):\n",
    "    \"\"\"Trains multiple machine learning models and applies the selected fairness constraint.\n",
    "    \n",
    "    This function trains four different models, applies fairness constraints, and evaluates their \n",
    "    performance both before and after bias mitigation.\n",
    "    \n",
    "    Args:\n",
    "        data: The dataset as a pandas DataFrame.\n",
    "        target_column: The column name of the target variable.\n",
    "        sensitive_feature_column: The column name of the sensitive feature.\n",
    "        constraint_option: The fairness constraint to be applied (Demographic Parity or Equalized Odds).\n",
    "    \"\"\"\n",
    "    # Define features (X) and the target variable (y) for the models\n",
    "    X = data.drop(target_column, axis=1)\n",
    "    y = data[target_column]\n",
    "    sensitive_feature = data[sensitive_feature_column]\n",
    "\n",
    "    # Define the machine learning models to be trained\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(),\n",
    "        \"Random Forest\": RandomForestClassifier(),\n",
    "        \"Gradient Boosting\": GradientBoostingClassifier()\n",
    "    }\n",
    "\n",
    "    # Train and evaluate each model separately\n",
    "    for model_name, model in models.items():\n",
    "        st.markdown(f\"## {model_name}\")\n",
    "        X_train, X_test, y_train, y_test, sensitive_train, sensitive_test = train_test_split(\n",
    "            X, y, sensitive_feature, test_size=0.25, random_state=42)\n",
    "\n",
    "        # Train the model on the training set\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions on the testing set\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate initial fairness metrics before bias mitigation\n",
    "        metrics = {\n",
    "            'selection_rate': selection_rate,\n",
    "            'true_positive_rate': true_positive_rate,\n",
    "            'true_negative_rate': true_negative_rate,\n",
    "            'false_positive_rate': false_positive_rate,\n",
    "            'false_negative_rate': false_negative_rate\n",
    "        }\n",
    "        metric_frame = MetricFrame(metrics=metrics, y_true=y_test, y_pred=y_pred, sensitive_features=sensitive_test)\n",
    "        sr_by_group = metric_frame.by_group['selection_rate']\n",
    "        disparate_impact = sr_by_group.min() / sr_by_group.max()\n",
    "        bias_percentage = (1 - disparate_impact) * 100\n",
    "\n",
    "        # Display the original accuracy and fairness metrics\n",
    "        st.write(f\"**Original Accuracy**: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "        st.write(f\"**Original Bias Percentage**: {bias_percentage:.4f}%\")\n",
    "        st.write(f\"**Original Disparate Impact**: {disparate_impact:.4f}\")\n",
    "\n",
    "        # Apply the selected fairness constraint to mitigate bias\n",
    "        if constraint_option == \"Demographic Parity\":\n",
    "            constraint = DemographicParity()\n",
    "        else:\n",
    "            constraint = EqualizedOdds()\n",
    "\n",
    "        # Use ExponentiatedGradient to apply the fairness constraint and retrain the model\n",
    "        mitigator = ExponentiatedGradient(model, constraint)\n",
    "        mitigator.fit(X_train, y_train, sensitive_features=sensitive_train)\n",
    "        y_pred_mitigated = mitigator.predict(X_test)\n",
    "\n",
    "        # Calculate metrics after bias mitigation\n",
    "        accuracy_mitigated = accuracy_score(y_test, y_pred_mitigated)\n",
    "        precision = precision_score(y_test, y_pred_mitigated)\n",
    "        recall = recall_score(y_test, y_pred_mitigated)\n",
    "        f1 = f1_score(y_test, y_pred_mitigated)\n",
    "\n",
    "        # Evaluate fairness metrics after bias mitigation\n",
    "        metric_frame_mitigated = MetricFrame(metrics=metrics, y_true=y_test, y_pred=y_pred_mitigated, sensitive_features=sensitive_test)\n",
    "        sr_by_group_mitigated = metric_frame_mitigated.by_group['selection_rate']\n",
    "        disparate_impact_mitigated = sr_by_group_mitigated.min() / sr_by_group_mitigated.max()\n",
    "        bias_percentage_mitigated = (1 - disparate_impact_mitigated) * 100\n",
    "\n",
    "        # Display the mitigated accuracy and fairness metrics\n",
    "        st.write(f\"**Mitigated Accuracy**: {accuracy_mitigated:.4f}\")\n",
    "        st.write(f\"**Mitigated Bias Percentage**: {bias_percentage_mitigated:.4f}%\")\n",
    "        st.write(f\"**Mitigated Disparate Impact**: {disparate_impact_mitigated:.4f}\")\n",
    "        st.write(f\"**Precision**: {precision:.4f}\")\n",
    "        st.write(f\"**Recall**: {recall:.4f}\")\n",
    "        st.write(f\"**F1 Score**: {f1:.4f}\")\n",
    "        st.markdown(\"---\")  # Divider between models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e138636",
   "metadata": {},
   "source": [
    "### StreamLit App Interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f130766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Streamlit app function to run the entire application\n",
    "def main():\n",
    "    # Display the \"About\" section at the top\n",
    "    about_section()\n",
    "    \n",
    "    # Title of the Streamlit app\n",
    "    st.title(\"Flexible Fairness Analysis Tool\")\n",
    "\n",
    "    # Upload CSV file\n",
    "    uploaded_file = st.file_uploader(\"Choose a CSV file\", type=\"csv\")\n",
    "\n",
    "    if uploaded_file is not None:\n",
    "        # Load and preprocess the uploaded data\n",
    "        data = load_and_preprocess_data(uploaded_file)\n",
    "        if data is not None:\n",
    "            # Display an overview of the dataset\n",
    "            st.write(\"### Dataset Overview\")\n",
    "            st.dataframe(data.head())\n",
    "\n",
    "            # Allow the user to select the target and sensitive feature columns\n",
    "            st.write(\"## Select Columns for Analysis\")\n",
    "            target_column = st.selectbox(\"Select the Target Column (e.g., income, credit_risk)\", options=data.columns)\n",
    "            sensitive_feature_column = st.selectbox(\"Select the Sensitive Feature Column (e.g., gender, race)\", options=data.columns)\n",
    "\n",
    "            # Button to check bias using Logistic Regression\n",
    "            check_bias = st.button(\"Check Bias\")\n",
    "            if check_bias or st.session_state.get('bias_checked', False):\n",
    "                if not st.session_state.get('bias_checked', False):\n",
    "                    # Encode the data and perform initial bias analysis\n",
    "                    data_encoded, sensitive_feature_mapping = encode_columns(data, sensitive_feature_column)\n",
    "                    bias_results = check_bias_logistic_regression(data_encoded, target_column, sensitive_feature_column, sensitive_feature_mapping)\n",
    "                    \n",
    "                    # Display the results of the bias analysis\n",
    "                    st.write(\"## Bias Analysis Results (Logistic Regression)\")\n",
    "                    st.write(f\"Disparate Impact: {bias_results['Disparate Impact']:.2f}\")\n",
    "                    st.write(f\"Bias Percentage: {bias_results['Bias Percentage']:.2f}%\")\n",
    "                    \n",
    "                    # Store the state after bias checking\n",
    "                    st.session_state['bias_checked'] = True\n",
    "                    st.session_state['data_encoded'] = data_encoded\n",
    "                    st.session_state['target_column'] = target_column\n",
    "                    st.session_state['sensitive_feature_column'] = sensitive_feature_column\n",
    "                    st.session_state['sensitive_feature_mapping'] = sensitive_feature_mapping\n",
    "\n",
    "                # Allow the user to choose a fairness constraint for bias mitigation\n",
    "                st.write(\"## Select Fairness Constraint for Bias Mitigation\")\n",
    "                constraint_option = st.selectbox(\"Select Fairness Constraint\", [\"Demographic Parity\", \"Equalized Odds\"])\n",
    "\n",
    "                # Button to apply the selected fairness constraint to all models\n",
    "                apply_fairness = st.button(\"Apply Fairness Constraint to All Models\")\n",
    "                if apply_fairness:\n",
    "                    # Retrieve the encoded data and column selections from the session state\n",
    "                    data_encoded = st.session_state['data_encoded']\n",
    "                    target_column = st.session_state['target_column']\n",
    "                    sensitive_feature_column = st.session_state['sensitive_feature_column']\n",
    "                    sensitive_feature_mapping = st.session_state['sensitive_feature_mapping']\n",
    "                    \n",
    "                    # Apply fairness constraints and display the results\n",
    "                    apply_fairness_constraints(data_encoded, target_column, sensitive_feature_column, constraint_option)\n",
    "\n",
    "# Run the app\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bbd165",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
