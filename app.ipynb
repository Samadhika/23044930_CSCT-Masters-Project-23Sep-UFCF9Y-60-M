{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e97ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8942ec0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbe4aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from fairlearn.metrics import MetricFrame, selection_rate, true_positive_rate, true_negative_rate, false_positive_rate, false_negative_rate\n",
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity, EqualizedOdds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2381ba2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355174fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic patch for np.PINF in fairlearn\n",
    "import fairlearn.reductions._exponentiated_gradient.exponentiated_gradient as eg\n",
    "if hasattr(np, 'PINF'):\n",
    "    np.PINF = np.inf  # Just in case, though it should already be removed\n",
    "eg.np.PINF = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61456650",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1beb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess the data\n",
    "def load_and_preprocess_data(uploaded_file):\n",
    "    try:\n",
    "        data = pd.read_csv(uploaded_file)\n",
    "    except pd.errors.EmptyDataError:\n",
    "        st.error(\"The uploaded file is empty or malformed.\")\n",
    "        return None\n",
    "\n",
    "    # Display the columns of the uploaded file for debugging\n",
    "    st.write(\"Columns in the uploaded file:\", data.columns.tolist())\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eabc21e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2070ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode categorical columns and retain the mapping for the sensitive feature\n",
    "def encode_columns(data, sensitive_feature_column):\n",
    "    # Encode the entire dataframe except the sensitive feature\n",
    "    data_encoded = data.copy()\n",
    "    for column in data_encoded.columns:\n",
    "        if data_encoded[column].dtype == object and column != sensitive_feature_column:\n",
    "            data_encoded[column] = LabelEncoder().fit_transform(data_encoded[column])\n",
    "\n",
    "    # Encode the sensitive feature separately and store the mapping\n",
    "    le = LabelEncoder()\n",
    "    data_encoded[sensitive_feature_column] = le.fit_transform(data_encoded[sensitive_feature_column])\n",
    "    sensitive_feature_mapping = dict(zip(le.transform(le.classes_), le.classes_))\n",
    "\n",
    "    return data_encoded, sensitive_feature_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbbbe69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cdec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model and evaluate bias\n",
    "def train_and_evaluate_bias(data, target_column, sensitive_feature_column):\n",
    "    try:\n",
    "        X = data.drop(target_column, axis=1)\n",
    "        y = data[target_column]\n",
    "        sensitive_feature = data[sensitive_feature_column]\n",
    "\n",
    "        X_train, X_test, y_train, y_test, sensitive_train, sensitive_test = train_test_split(X, y, sensitive_feature, test_size=0.25, random_state=42)\n",
    "\n",
    "        model = LogisticRegression(max_iter=1000)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Define a dictionary of metrics\n",
    "        metrics = {\n",
    "            'selection_rate': selection_rate,\n",
    "            'true_positive_rate': true_positive_rate,\n",
    "            'true_negative_rate': true_negative_rate,\n",
    "            'false_positive_rate': false_positive_rate,\n",
    "            'false_negative_rate': false_negative_rate\n",
    "        }\n",
    "\n",
    "        # Evaluate the metrics\n",
    "        metric_frame = MetricFrame(metrics=metrics, y_true=y_test, y_pred=y_pred, sensitive_features=sensitive_test)\n",
    "\n",
    "        sr_by_group = metric_frame.by_group['selection_rate']\n",
    "        disparate_impact = sr_by_group.min() / sr_by_group.max()\n",
    "        bias_percentage = (1 - disparate_impact) * 100\n",
    "\n",
    "        return metric_frame, disparate_impact, bias_percentage, X_train, X_test, y_train, y_test, sensitive_train, sensitive_test, model\n",
    "\n",
    "    except Exception as e:\n",
    "        st.error(\"Please select applicable Target Column and Sensitive Feature Column.\")\n",
    "        return None, None, None, None, None, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43c810a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1f962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply fairness constraints and evaluate additional metrics\n",
    "def apply_fairness_constraints_and_metrics(constraint_option, model, X_train, X_test, y_train, y_test, sensitive_train, sensitive_test):\n",
    "    if constraint_option == \"Disparate Impact Mitigation\":\n",
    "        constraint = DemographicParity()\n",
    "    else:\n",
    "        constraint = EqualizedOdds()\n",
    "\n",
    "    mitigator = ExponentiatedGradient(model, constraint)\n",
    "    mitigator.fit(X_train, y_train, sensitive_features=sensitive_train)\n",
    "\n",
    "    y_pred_mitigated = mitigator.predict(X_test)\n",
    "    accuracy_mitigated = accuracy_score(y_test, y_pred_mitigated)\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    precision = precision_score(y_test, y_pred_mitigated)\n",
    "    recall = recall_score(y_test, y_pred_mitigated)\n",
    "    f1 = f1_score(y_test, y_pred_mitigated)\n",
    "\n",
    "    # Define a dictionary of metrics for the mitigated model\n",
    "    metrics_mitigated = {\n",
    "        'selection_rate': selection_rate,\n",
    "        'true_positive_rate': true_positive_rate,\n",
    "        'true_negative_rate': true_negative_rate,\n",
    "        'false_positive_rate': false_positive_rate,\n",
    "        'false_negative_rate': false_negative_rate\n",
    "    }\n",
    "\n",
    "    metric_frame_mitigated = MetricFrame(metrics=metrics_mitigated, y_true=y_test, y_pred=y_pred_mitigated, sensitive_features=sensitive_test)\n",
    "\n",
    "    sr_by_group_mitigated = metric_frame_mitigated.by_group['selection_rate']\n",
    "    disparate_impact_mitigated = sr_by_group_mitigated.min() / sr_by_group_mitigated.max()\n",
    "    bias_percentage_mitigated = (1 - disparate_impact_mitigated) * 100\n",
    "\n",
    "    return accuracy_mitigated, metric_frame_mitigated, disparate_impact_mitigated, bias_percentage_mitigated, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18be4fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b325d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display the results with original category names\n",
    "def display_results(disparate_impact, bias_percentage, metric_frame, sensitive_feature_mapping, accuracy_mitigated=None, metric_frame_mitigated=None, disparate_impact_mitigated=None, bias_percentage_mitigated=None, precision=None, recall=None, f1=None):\n",
    "    st.write(f\"Disparate Impact: {disparate_impact:.2f}\")\n",
    "    st.write(f\"Bias Percentage: {bias_percentage:.2f}%\")\n",
    "    \n",
    "    st.write(\"Overall metrics:\")\n",
    "    st.write(metric_frame.overall)\n",
    "\n",
    "    st.write(\"Metrics by group:\")\n",
    "    metrics_by_group = metric_frame.by_group.rename(index=sensitive_feature_mapping)\n",
    "    st.write(metrics_by_group)\n",
    "\n",
    "    if accuracy_mitigated is not None and metric_frame_mitigated is not None:\n",
    "        st.write(f\"Mitigated Model accuracy: {accuracy_mitigated:.2f}\")\n",
    "        st.write(\"Overall metrics for mitigated model:\")\n",
    "        st.write(metric_frame_mitigated.overall)\n",
    "\n",
    "        st.write(\"Metrics by group for mitigated model:\")\n",
    "        metrics_by_group_mitigated = metric_frame_mitigated.by_group.rename(index=sensitive_feature_mapping)\n",
    "        st.write(metrics_by_group_mitigated)\n",
    "\n",
    "        st.write(f\"Mitigated Disparate Impact: {disparate_impact_mitigated:.2f}\")\n",
    "        st.write(f\"Mitigated Bias Percentage: {bias_percentage_mitigated:.2f}%\")\n",
    "\n",
    "        # Display additional metrics\n",
    "        st.write(\"### Additional Metrics for Mitigated Model\")\n",
    "        st.write(f\"Precision: {precision:.2f}\")\n",
    "        st.write(f\"Recall: {recall:.2f}\")\n",
    "        st.write(f\"F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfe8c77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52af89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Streamlit app function\n",
    "def main():\n",
    "    st.title(\"Flexible Fairness Analysis Tool\")\n",
    "\n",
    "    uploaded_file = st.file_uploader(\"Choose a CSV file\", type=\"csv\")\n",
    "\n",
    "    if uploaded_file is not None:\n",
    "        # Load and preprocess the data\n",
    "        data = load_and_preprocess_data(uploaded_file)\n",
    "\n",
    "        if data is not None:\n",
    "            st.write(\"### Dataset Overview\")\n",
    "            st.dataframe(data.head())\n",
    "\n",
    "            # Let the user select the target and sensitive feature columns\n",
    "            st.write(\"## Select Columns for Analysis\")\n",
    "            target_column = st.selectbox(\"Select the Target Column (e.g., income, credit_risk)\", options=data.columns)\n",
    "            sensitive_feature_column = st.selectbox(\"Select the Sensitive Feature Column (e.g., gender, race)\", options=data.columns)\n",
    "\n",
    "            # Encode the data and retain the mapping for the sensitive feature\n",
    "            data_encoded, sensitive_feature_mapping = encode_columns(data, sensitive_feature_column)\n",
    "\n",
    "            # Train and evaluate bias\n",
    "            result = train_and_evaluate_bias(data_encoded, target_column=target_column, sensitive_feature_column=sensitive_feature_column)\n",
    "            if result[0] is not None:\n",
    "                metric_frame, disparate_impact, bias_percentage, X_train, X_test, y_train, y_test, sensitive_train, sensitive_test, model = result\n",
    "\n",
    "                # Display initial results with original category names\n",
    "                display_results(disparate_impact, bias_percentage, metric_frame, sensitive_feature_mapping)\n",
    "\n",
    "                # User selects a fairness constraint method\n",
    "                st.write(\"## Select Fairness Constraint for Further Analysis\")\n",
    "                constraint_option = st.selectbox(\"Select Fairness Constraint\", [\"Disparate Impact Mitigation\", \"Equalized Odds\"])\n",
    "\n",
    "                proceed = st.button(\"Proceed with Fairness Analysis\")\n",
    "\n",
    "                if proceed:\n",
    "                    st.write(\"## Fairness-Constrained Model Analysis\")\n",
    "\n",
    "                    # Apply the selected fairness constraint and evaluate additional metrics\n",
    "                    accuracy_mitigated, metric_frame_mitigated, disparate_impact_mitigated, bias_percentage_mitigated, precision, recall, f1 = apply_fairness_constraints_and_metrics(constraint_option, model, X_train, X_test, y_train, y_test, sensitive_train, sensitive_test)\n",
    "\n",
    "                    # Display fairness-constrained results with original category names and additional metrics\n",
    "                    display_results(disparate_impact, bias_percentage, metric_frame, sensitive_feature_mapping, accuracy_mitigated, metric_frame_mitigated, disparate_impact_mitigated, bias_percentage_mitigated, precision, recall, f1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
